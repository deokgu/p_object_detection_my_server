{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BZ4Yix498ZSZ"
   },
   "outputs": [],
   "source": [
    "# !pip install effdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hkfyUqF_4kD2"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈 import\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "suKoZTnb4kEC"
   },
   "outputs": [],
   "source": [
    "# CustomDataset class 선언\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # coco annotation 불러오기 (by. coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # boxes (x, y, w, h)\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        # box별 label\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            while True:\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    target['labels'] = torch.tensor(sample['labels'])\n",
    "                    break\n",
    "            \n",
    "        return image, target, image_id, image_info['file_name']\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KnxeE-VC4kED"
   },
   "outputs": [],
   "source": [
    "# Albumentation을 이용, augmentation 선언\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X4X6Sbmj4kEE"
   },
   "outputs": [],
   "source": [
    "from effdet import DetBenchPredict\n",
    "import gc\n",
    "\n",
    "# Effdet config를 통해 모델 불러오기 + ckpt load\n",
    "def load_net(checkpoint_path=None,):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    config.num_classes = 10\n",
    "    config.image_size = (512,512)\n",
    "    \n",
    "    config.soft_nms = False\n",
    "    config.max_det_per_image = 25\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "    \n",
    "    net = DetBenchPredict(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(checkpoint_path=None):\n",
    "    \n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    config.num_classes = 10\n",
    "    config.image_size = (512,512)\n",
    "    \n",
    "    config.soft_nms = False\n",
    "    config.max_det_per_image = 25\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "    return DetBenchTrain(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_net()\n",
    "b = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'effdet.bench.DetBenchTrain'>\n",
      "<class 'effdet.bench.DetBenchTrain'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a))\n",
    "print(type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4nvREUBg4kEF"
   },
   "outputs": [],
   "source": [
    "# valid function\n",
    "def valid_fn(val_data_loader, model, device):\n",
    "    outputs = []\n",
    "    for images, image_ids in tqdm(val_data_loader):\n",
    "        # gpu 계산을 위해 image.to(device)       \n",
    "        images = torch.stack(images) # bs, ch, w, h \n",
    "        images = images.to(device).float()\n",
    "        output = model(images)\n",
    "        for out in output:\n",
    "            outputs.append({'boxes': out.detach().cpu().numpy()[:,:4], \n",
    "                            'scores': out.detach().cpu().numpy()[:,4], \n",
    "                            'labels': out.detach().cpu().numpy()[:,-1]})\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from map_boxes import mean_average_precision_for_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tqwNIys14kEG"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    annotation = '/opt/ml/detection/dataset/train.json'\n",
    "    data_dir = '/opt/ml/detection/dataset'\n",
    "    val_dataset = CustomDataset(annotation, data_dir, get_valid_transform())\n",
    "    # epoch = \n",
    "    checkpoint_path = f'epoch_26_tf_efficientdet_d1.pth'\n",
    "    score_threshold = 0.1\n",
    "    val_data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    model = load_net(checkpoint_path, device)\n",
    "    \n",
    "    outputs = []\n",
    "    for images, targets, image_ids, filename in tqdm(val_data_loader):\n",
    "        # gpu 계산을 위해 image.to(device)       \n",
    "        images = torch.stack(images) # bs, ch, w, h \n",
    "        images = images.to(device).float()\n",
    "        output = model(images)\n",
    "        for out in output:\n",
    "            outputs.append({'boxes': out.detach().cpu().numpy()[:,:4], \n",
    "                            'scores': out.detach().cpu().numpy()[:,4], \n",
    "                            'labels': out.detach().cpu().numpy()[:,-1]})  \n",
    "        new_pred = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
    "                if score > score_threshold:\n",
    "                    new_pred.append([filename[i], int(label), score, box[0]*2, box[2]*2, box[1]*2, box[3]*2])\n",
    "        gt = []\n",
    "        for i, target in enumerate(targets):\n",
    "            bbox = target[\"boxes\"][i]\n",
    "            print(bbox)\n",
    "            gt.append([filename[i], int(target[\"labels\"][i]), bbox[1].item(), bbox[3].item(), bbox[0].item(), bbox[2].item()])\n",
    "        mean_ap, _ = mean_average_precision_for_boxes(gt, new_pred, iou_threshold=0.5)\n",
    "        \n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(f'submission_effcient_det_d3_50_train_shffle_True.csv', index=None)\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1jFw--vm4kEI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4883 [00:00<?, ?it/s]/opt/conda/envs/detection/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  0%|          | 0/4883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 6, 0.2618692, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 6, 0.2618692, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 5, 0.24678436, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 6, 0.2618692, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 5, 0.24678436, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 3, 0.2012389, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 6, 0.2618692, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 5, 0.24678436, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 3, 0.2012389, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 7, 0.18231925, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "[['train/0000.jpg', 1, 0.30246696, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 2, 0.264304, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 6, 0.2618692, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 5, 0.24678436, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 3, 0.2012389, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 7, 0.18231925, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125], ['train/0000.jpg', 9, 0.13984144, 206.68402099609375, 743.9473266601562, 201.6585693359375, 717.5313720703125]]\n",
      "tensor([193.7000, 197.6000, 663.4000, 745.4000], dtype=torch.float64)\n",
      "[['train/0000.jpg', 0, 197.6, 745.4, 193.7, 663.4]]\n",
      "Number of files in annotations: 1\n",
      "Number of files in predictions: 1\n",
      "Unique classes: 1\n",
      "Detections length: 1\n",
      "Annotations length: 1\n",
      "0                              | 0.000000 |       1\n",
      "mAP: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientDet_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
