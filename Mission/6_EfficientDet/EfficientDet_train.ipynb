{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"detection","language":"python","name":"detection"},"colab":{"name":"EfficientDet_train.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","execution_count":1,"source":["!pip install effdet"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: effdet in /opt/conda/envs/detection/lib/python3.7/site-packages (0.2.4)\n","Requirement already satisfied: pycocotools>=2.0.2 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (2.0.2)\n","Requirement already satisfied: torchvision in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (0.8.2)\n","Requirement already satisfied: timm>=0.3.2 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (0.4.12)\n","Requirement already satisfied: omegaconf>=2.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (2.1.1)\n","Requirement already satisfied: torch>=1.4 in /opt/conda/envs/detection/lib/python3.7/site-packages (from effdet) (1.7.1)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /opt/conda/envs/detection/lib/python3.7/site-packages (from omegaconf>=2.0->effdet) (4.8)\n","Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from omegaconf>=2.0->effdet) (5.4.1)\n","Requirement already satisfied: setuptools>=18.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from pycocotools>=2.0.2->effdet) (58.0.4)\n","Requirement already satisfied: cython>=0.27.3 in /opt/conda/envs/detection/lib/python3.7/site-packages (from pycocotools>=2.0.2->effdet) (0.29.24)\n","Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from pycocotools>=2.0.2->effdet) (3.4.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (0.10.0)\n","Requirement already satisfied: numpy>=1.16 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.21.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.3.2)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/detection/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (8.3.2)\n","Requirement already satisfied: six in /opt/conda/envs/detection/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.16.0)\n","Requirement already satisfied: typing_extensions in /opt/conda/envs/detection/lib/python3.7/site-packages (from torch>=1.4->effdet) (3.10.0.2)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":810},"id":"O_tnPGK74wUM","executionInfo":{"status":"ok","timestamp":1632790350870,"user_tz":-540,"elapsed":6955,"user":{"displayName":"남혜린","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij4ympK92NeDVGbptWeMlA_CJqkEdmffpIl0WH=s64","userId":"04952951786569469377"}},"outputId":"c6e7e511-cd30-4b08-ec0f-76be2eeb609d"}},{"cell_type":"code","execution_count":2,"source":["# 라이브러리 및 모듈 import\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import numpy as np\n","import cv2\n","import os\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n","from effdet.efficientdet import HeadNet\n","import pandas as pd\n","from tqdm import tqdm"],"outputs":[],"metadata":{"id":"n5QtMnjy4g2y"}},{"cell_type":"code","execution_count":3,"source":["# CustomDataset class 선언\n","\n","class CustomDataset(Dataset):\n","    '''\n","      data_dir: data가 존재하는 폴더 경로\n","      transforms: data transform (resize, crop, Totensor, etc,,,)\n","    '''\n","\n","    def __init__(self, annotation, data_dir, transforms=None):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        \n","        # coco annotation 불러오기 (by. coco API)\n","        self.coco = COCO(annotation)\n","        self.predictions = {\n","            \"images\": self.coco.dataset[\"images\"].copy(),\n","            \"categories\": self.coco.dataset[\"categories\"].copy(),\n","            \"annotations\": None\n","        }\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.coco.getImgIds(imgIds=index)\n","\n","        image_info = self.coco.loadImgs(image_id)[0]\n","        \n","        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n","        anns = self.coco.loadAnns(ann_ids)\n","\n","        # boxes (x, y, w, h)\n","        boxes = np.array([x['bbox'] for x in anns])\n","\n","        # boxex (x_min, y_min, x_max, y_max)\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        \n","        # box별 label\n","        labels = np.array([x['category_id'] for x in anns])\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        \n","        areas = np.array([x['area'] for x in anns])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        \n","        is_crowds = np.array([x['iscrowd'] for x in anns])\n","        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n","\n","        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n","                  'iscrowd': is_crowds}\n","\n","        # transform\n","        if self.transforms:\n","            while True:\n","                sample = self.transforms(**{\n","                    'image': image,\n","                    'bboxes': target['boxes'],\n","                    'labels': labels\n","                })\n","                if len(sample['bboxes']) > 0:\n","                    image = sample['image']\n","                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n","                    target['labels'] = torch.tensor(sample['labels'])\n","                    break\n","            \n","        return image, target, image_id\n","    \n","    def __len__(self) -> int:\n","        return len(self.coco.getImgIds())"],"outputs":[],"metadata":{"id":"Xu7QfZ-14g28"}},{"cell_type":"code","execution_count":4,"source":["# Albumentation을 이용, augmentation 선언\n","def get_train_transform():\n","    return A.Compose([\n","        A.Resize(512, 512),\n","        A.Flip(p=0.5),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"],"outputs":[],"metadata":{"id":"k_mYQSta4g2_"}},{"cell_type":"code","execution_count":5,"source":["# loss 추적\n","class Averager:\n","    def __init__(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","    def send(self, value):\n","        self.current_total += value\n","        self.iterations += 1\n","\n","    @property\n","    def value(self):\n","        if self.iterations == 0:\n","            return 0\n","        else:\n","            return 1.0 * self.current_total / self.iterations\n","\n","    def reset(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))"],"outputs":[],"metadata":{"id":"Lx1ih23h4g3C"}},{"cell_type":"code","execution_count":6,"source":["# Effdet config\n","# https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/config/model_config.py\n","\n","# Effdet config를 통해 모델 불러오기\n","def get_net(checkpoint_path=None):\n","    \n","    config = get_efficientdet_config('tf_efficientdet_d1')\n","    config.num_classes = 10\n","    config.image_size = (512,512)\n","    \n","    config.soft_nms = False\n","    config.max_det_per_image = 25\n","    \n","    net = EfficientDet(config, pretrained_backbone=True)\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n","    \n","    if checkpoint_path:\n","        checkpoint = torch.load(checkpoint_path)\n","        net.load_state_dict(checkpoint['model_state_dict'])\n","        \n","    return DetBenchTrain(net)\n","    \n","# train function\n","def train_fn(num_epochs, train_data_loader, optimizer, model, device, clip=35):\n","    loss_hist = Averager()\n","    model.train()\n","    \n","    for epoch in range(num_epochs):\n","        loss_hist.reset()\n","        \n","        for images, targets, image_ids in tqdm(train_data_loader):\n","            \n","                images = torch.stack(images) # bs, ch, w, h - 16, 3, 512, 512\n","                images = images.to(device).float()\n","                boxes = [target['boxes'].to(device).float() for target in targets]\n","                labels = [target['labels'].to(device).float() for target in targets]\n","                target = {\"bbox\": boxes, \"cls\": labels}\n","\n","                # calculate loss\n","                loss, cls_loss, box_loss = model(images, target).values()\n","                loss_value = loss.detach().item()\n","                \n","                loss_hist.send(loss_value)\n","                \n","                # backward\n","                optimizer.zero_grad()\n","                loss.backward()\n","                # grad clip\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","                \n","                optimizer.step()\n","\n","        print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n","        torch.save(model.state_dict(), f'epoch_{epoch+1}.pth')"],"outputs":[],"metadata":{"id":"ERF2BAfo4g3D"}},{"cell_type":"code","execution_count":7,"source":["def main():\n","    annotation = '/opt/ml/detection/dataset/train.json'\n","    data_dir = '/opt/ml/detection/dataset'\n","    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n","\n","    train_data_loader = DataLoader(\n","        train_dataset,\n","        batch_size=16,\n","        shuffle=True,\n","        num_workers=4,\n","        collate_fn=collate_fn\n","    )\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    print(device)\n","\n","    model = get_net()\n","    model.to(device)\n","    \n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","    num_epochs = 50\n","\n","    loss = train_fn(num_epochs, train_data_loader, optimizer, model, device)"],"outputs":[],"metadata":{"id":"0mbJK-ae4g3E"}},{"cell_type":"code","execution_count":8,"source":["if __name__ == '__main__':\n","    main()"],"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.12s)\n","creating index...\n","index created!\n","cuda\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:20<00:00,  1.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #1 loss: 24.672096506442898\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:46<00:00,  1.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #2 loss: 1.2393232330777286\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:03<00:00,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #3 loss: 1.1901964334880604\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:00<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #4 loss: 1.1335169084711012\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:07<00:00,  1.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #5 loss: 1.0798889819313497\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:06<00:00,  1.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #6 loss: 1.040358827589384\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:38<00:00,  1.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #7 loss: 1.0008541647515266\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:36<00:00,  1.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #8 loss: 0.9622795272107217\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:11<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #9 loss: 0.9304494438997282\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:11<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #10 loss: 0.9012989314163432\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:05<00:00,  1.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #11 loss: 0.8814331213243647\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:02<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #12 loss: 0.8590726234944038\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:34<00:00,  1.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #13 loss: 0.83961072077159\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:33<00:00,  1.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #14 loss: 0.8235022332154068\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:00<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #15 loss: 0.8098320421440149\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:00<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #16 loss: 0.7927161910954643\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:44<00:00,  1.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #17 loss: 0.777351993945689\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:22<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #18 loss: 0.7701639739516514\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:00<00:00,  1.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #19 loss: 0.758304628087025\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:42<00:00,  1.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #20 loss: 0.7461642058067073\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:24<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #21 loss: 0.7386527211447946\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:32<00:00,  1.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #22 loss: 0.7280844263001984\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:34<00:00,  1.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #23 loss: 0.7185200425534467\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:01<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #24 loss: 0.7120456350784675\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:04<00:00,  1.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #25 loss: 0.701209503451204\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [03:15<00:00,  1.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch #26 loss: 0.6952910610273773\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 151/306 [01:55<01:57,  1.32it/s]"]}],"metadata":{"scrolled":true,"id":"UuCiT5tZ4g3G","outputId":"cb631ed3-95a0-4d9b-e0ac-c2a6e45c33a5"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}