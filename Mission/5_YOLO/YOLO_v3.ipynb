{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"YOLO_v3.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZuPYUxAkYGbu"},"source":["### git repository 불러오기 및 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"uluDnaUJYGcA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632787689683,"user_tz":-540,"elapsed":620,"user":{"displayName":"남혜린","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij4ympK92NeDVGbptWeMlA_CJqkEdmffpIl0WH=s64","userId":"04952951786569469377"}},"outputId":"9b003fc1-a09e-4667-f7b1-3bea8b43ba02"},"source":["!git clone https://github.com/ultralytics/yolov3.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'yolov3' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","metadata":{"id":"BO5tAS_TyHBE"},"source":["!pip install seaborn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFxpFiU9YGcH"},"source":["### Library 불러오기"]},{"cell_type":"code","metadata":{"id":"HTi61U_nYGcO"},"source":["import sys\n","import os\n","sys.path.append('./yolov3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6JrVKQumYGcS"},"source":["from pycocotools.coco import COCO\n","import numpy as np\n","import cv2\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from utils.loss import ComputeLoss\n","\n","import yaml\n","import torch\n","from models.yolo import Model\n","from pycocotools.cocoeval import COCOeval\n","import pandas as pd\n","\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n","    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4kfCB5lYGcV"},"source":["### Custom 데이터셋 정의"]},{"cell_type":"code","metadata":{"id":"kW89Amh3YGcY"},"source":["class CustomDataset(Dataset):\n","    '''\n","      data_dir: data가 존재하는 폴더 경로\n","      transforms: data transform (resize, crop, Totensor, etc,,,)\n","    '''\n","\n","    def __init__(self, annotation, data_dir, transforms=None, image_size=512):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.coco = COCO(annotation)\n","        self.image_size = image_size\n","        self.predictions = {\n","            \"images\": self.coco.dataset[\"images\"].copy(),\n","            \"categories\": self.coco.dataset[\"categories\"].copy(),\n","            \"annotations\": None\n","        }\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.coco.getImgIds(imgIds=index)\n","        image_info = self.coco.loadImgs(image_id)[0]\n","        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n","        anns = self.coco.loadAnns(ann_ids)\n","\n","        # boxes (x, y, w, h)\n","        boxes = np.array([x['bbox'] for x in anns])\n","\n","        # boxes (x_min, y_min, x_max, y_max)\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        boxes[:, 0] /= int(self.image_size)\n","        boxes[:, 1] /= int(self.image_size)\n","        boxes[:, 2] /= int(self.image_size)\n","        boxes[:, 3] /= int(self.image_size)\n","        \n","        labels = list([x['category_id'] for x in anns])\n","        areas = np.array([x['area'] for x in anns])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        \n","        is_crowds = np.array([x['iscrowd'] for x in anns])\n","        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n","\n","        \n","        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n","                  'iscrowd': is_crowds}\n","\n","        # transform\n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","            target['boxes'] = [list(box) for box in sample['bboxes']]\n","\n","        result = []\n","        labels_out = torch.zeros((len(labels), 6))\n","        for i in range(len(labels)):\n","            result.append(\n","                [labels[i], target['boxes'][i][0], target['boxes'][i][1], target['boxes'][i][2], target['boxes'][i][3]])\n","        result = np.array(result)\n","\n","        if len(labels):\n","            labels_out[:, 1:] = torch.from_numpy(result)\n","\n","        return image, labels_out, image_id\n","    \n","    def __len__(self) -> int:\n","        return len(self.coco.getImgIds())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BInpIOlaYGcg"},"source":["def get_train_transform():\n","    return A.Compose([\n","        A.Resize(512,512),\n","        A.Flip(p=0.5),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOLE7E_-YGcj"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"NStG0qtrYGck"},"source":["class Averager:\n","    def __init__(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","    def send(self, value):\n","        self.current_total += value\n","        self.iterations += 1\n","\n","    @property\n","    def value(self):\n","        if self.iterations == 0:\n","            return 0\n","        else:\n","            return 1.0 * self.current_total / self.iterations\n","\n","    def reset(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","\n","def collate_fn(batch):\n","    img, label, image_id = zip(*batch)  # transposed\n","    for i, l in enumerate(label):\n","        l[:, 0] = i  # add target image index for build_targets()\n","    return torch.stack(img, 0), torch.cat(label, 0), image_id\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXnM0cNvYGcl"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"Dh0Vt6D3YGcq"},"source":["def train_fn(num_epochs, train_data_loader, optimizer, model, device):\n","    model.gr = 1.0\n","    itr = 1\n","    loss_hist = Averager()\n","    compute_loss = ComputeLoss(model)\n","    best_loss = 1000\n","    for epoch in range(num_epochs):\n","        loss_hist.reset()\n","\n","        for images, targets, image_ids in tqdm(train_data_loader):\n","            images = torch.as_tensor([image.numpy() for image in images]).to(device)\n","            pred = model(images)\n","\n","            losses, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n","\n","            loss_value = losses.item()\n","\n","            loss_hist.send(loss_value)\n","\n","            # backward\n","            optimizer.zero_grad()\n","            losses.backward()\n","            optimizer.step()\n","\n","\n","        print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n","        if loss_hist.value < best_loss:\n","            torch.save(model.state_dict(), f'./yolov3/weights/yolov3.pth')\n","            best_loss = loss_hist.value\n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eX3aSKJyYGcu"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"TcI2pwnfYGcv"},"source":["def main():\n","    data_dir = '../../dataset'\n","    annotation = '../../dataset/train.json'\n","    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n","\n","    train_data_loader = DataLoader(\n","        train_dataset,\n","        batch_size=8,\n","        shuffle=False,\n","        num_workers=4,\n","        collate_fn=collate_fn\n","    )\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    print(device)\n","    # load a model;\n","\n","    num_classes = 10\n","    model = Model(cfg='./yolov3/models/yolov3.yaml', ch=3, nc=num_classes)  # create\n","\n","    # get number of input features for the classifier\n","    model.to(device)\n","\n","    # Hyperparameters\n","    with open('./yolov3/data/hyp.scratch.yaml') as f:\n","        hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n","    model.hyp = hyp\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","    num_epochs = 10\n","\n","    train_fn(num_epochs, train_data_loader, optimizer, model, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcXMmIE5YGcw","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1632787731065,"user_tz":-540,"elapsed":423,"user":{"displayName":"남혜린","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij4ympK92NeDVGbptWeMlA_CJqkEdmffpIl0WH=s64","userId":"04952951786569469377"}},"outputId":"56ee58af-62ce-4479-ac6b-a02e332924d8"},"source":["if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-b48e43276993>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../dataset/train.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_train_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     train_data_loader = DataLoader(\n","\u001b[0;32m<ipython-input-9-ae5b2c28d75a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation, data_dir, transforms, image_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.predictions = {\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../dataset/train.json'"]}]},{"cell_type":"code","metadata":{"id":"bBc9MkAZu45P"},"source":[""],"execution_count":null,"outputs":[]}]}